# -*- coding: utf-8 -*-
"""DAAPractical.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ubfOveHwQI3a_rdHogRPIeB5XDrqZbgA

PART A: Implement Quicksort using divide and conquer strategy and display time for sorting
for 500 values.
PART B: Use same data for Mergesort and compare the time required with Quicksort.
"""

import random
import time

# Quicksort Implementation
def quicksort(arr):
    if len(arr) <= 1:
        return arr
    pivot = arr[len(arr) // 2]
    left = [x for x in arr if x < pivot]
    middle = [x for x in arr if x == pivot]
    right = [x for x in arr if x > pivot]
    return quicksort(left) + middle + quicksort(right)

# Mergesort Implementation
def mergesort(arr):
    if len(arr) <= 1:
        return arr
    mid = len(arr) // 2
    left = mergesort(arr[:mid])
    right = mergesort(arr[mid:])
    return merge(left, right)

def merge(left, right):
    result = []
    i = j = 0
    while i < len(left) and j < len(right):
        if left[i] < right[j]:
            result.append(left[i])
            i += 1
        else:
            result.append(right[j])
            j += 1
    result.extend(left[i:])
    result.extend(right[j:])
    return result

# Generate Random Data
data = [random.randint(1, 1000) for _ in range(500)]

# Quicksort Timing
start_time = time.time()
quicksorted_data = quicksort(data)
quick_time = time.time() - start_time

# Mergesort Timing
start_time = time.time()
mergesorted_data = mergesort(data)
merge_time = time.time() - start_time

# Output Results
print(f"Quicksort Time: {quick_time:.6f} seconds")
print(f"Mergesort Time: {merge_time:.6f} seconds")



"""### **Time and Space Complexity**
- **Quicksort**
  - Time Complexity:  
    - Best Case: \(O(n \log n)\)  
    - Worst Case: \(O(n^2)\) (when the pivot is poorly chosen)  
    - Average Case: \(O(n \log n)\)
  - Space Complexity: \(O(\log n)\) (due to recursion stack)

- **Mergesort**
  - Time Complexity:  
    - Best, Worst, and Average Case: \(O(n \log n)\)
  - Space Complexity: \(O(n)\) (due to auxiliary arrays)

---

PART A: Implement Quick Sort using divide and conquer strategy.
PART B: Compare its time required wrt Merge sort OR Randomized Quick Sort.
"""

import random
import time

# Quick Sort Implementation
def quick_sort(arr):
    if len(arr) <= 1:
        return arr
    pivot = arr[len(arr) // 2]
    left = [x for x in arr if x < pivot]
    middle = [x for x in arr if x == pivot]
    right = [x for x in arr if x > pivot]
    return quick_sort(left) + middle + quick_sort(right)

# Merge Sort Implementation
def merge_sort(arr):
    if len(arr) <= 1:
        return arr
    mid = len(arr) // 2
    left = merge_sort(arr[:mid])
    right = merge_sort(arr[mid:])
    return merge(left, right)

def merge(left, right):
    result = []
    i = j = 0
    while i < len(left) and j < len(right):
        if left[i] < right[j]:
            result.append(left[i])
            i += 1
        else:
            result.append(right[j])
            j += 1
    result.extend(left[i:])
    result.extend(right[j:])
    return result

# Generate Random Data
data = [random.randint(1, 1000) for _ in range(500)]

# Quick Sort Timing
start_time = time.time()
quick_sorted = quick_sort(data)
quick_time = time.time() - start_time

# Merge Sort Timing
start_time = time.time()
merge_sorted = merge_sort(data)
merge_time = time.time() - start_time

# Print Results
print(f"Quick Sort Time: {quick_time:.6f} seconds")
print(f"Merge Sort Time: {merge_time:.6f} seconds")

"""| Algorithm  | Best Case      | Worst Case      | Average Case    | Space Complexity |
|------------|----------------|-----------------|-----------------|------------------|
| Quick Sort | \(O(n \log n)\)| \(O(n^2)\)      | \(O(n \log n)\) | \(O(\log n)\)    |
| Merge Sort | \(O(n \log n)\)| \(O(n \log n)\) | \(O(n \log n)\) | \(O(n)\)         |

3 PART A: Implement Quick Sort using divide and conquer strategy. Give more than 500 inputs
for best and for worst case scenario, and compare the time taken.
PART B: Write a function to demonstrate Mutation of a chromosome representing solution of
Traveling Salesperson Problem (TSP)

Samples for TSP :
"""

import random
import time

# Quick Sort Implementation
def quick_sort(arr):
    if len(arr) <= 1:
        return arr
    pivot = arr[len(arr) // 2]
    left = [x for x in arr if x < pivot]
    middle = [x for x in arr if x == pivot]
    right = [x for x in arr if x > pivot]
    return quick_sort(left) + middle + quick_sort(right)

# Generate Input for Best Case
best_case_input = [random.randint(1, 1000) for _ in range(1000)]

# Generate Input for Worst Case
worst_case_input = sorted(best_case_input)

# Measure Time for Best Case
start_time = time.time()
quick_sort(best_case_input)
best_case_time = time.time() - start_time

# Measure Time for Worst Case
start_time = time.time()
quick_sort(worst_case_input)
worst_case_time = time.time() - start_time

# Results
print(f"Best Case Time: {best_case_time:.6f} seconds")
print(f"Worst Case Time: {worst_case_time:.6f} seconds")

# -----------------------------
import random

# Mutation Function
def mutate_chromosome(chromosome):
    # Choose two random positions in the chromosome
    i, j = random.sample(range(len(chromosome)), 2)
    # Swap the two cities
    chromosome[i], chromosome[j] = chromosome[j], chromosome[i]
    return chromosome

# Example Chromosome (TSP Route)
chromosome = [0, 1, 2, 3, 4, 5, 6, 7]

# Before Mutation
print("Original Chromosome:", chromosome)

# Perform Mutation
mutated_chromosome = mutate_chromosome(chromosome)

# After Mutation
print("Mutated Chromosome: ", mutated_chromosome)

"""### **Summary of Complexity**
| Part         | Scenario      | Time Complexity  | Space Complexity |
|--------------|---------------|------------------|------------------|
| Quick Sort   | Best Case     | \(O(n \log n)\)  | \(O(\log n)\)    |
| Quick Sort   | Worst Case    | \(O(n^2)\)       | \(O(n)\)         |
| TSP Mutation | Mutation Step | \(O(1)\)         | \(O(1)\)         |
"""

import random
import time

# Merge Sort Implementation
def merge_sort(arr):
    if len(arr) <= 1:
        return arr
    mid = len(arr) // 2
    left = merge_sort(arr[:mid])
    right = merge_sort(arr[mid:])
    return merge(left, right)

def merge(left, right):
    result = []
    i = j = 0
    while i < len(left) and j < len(right):
        if left[i] < right[j]:
            result.append(left[i])
            i += 1
        else:
            result.append(right[j])
            j += 1
    result.extend(left[i:])
    result.extend(right[j:])
    return result

# Generate Best Case Input (Randomized Array)
best_case_input = [random.randint(1, 1000) for _ in range(10000)]

# Generate Worst Case Input (Already Sorted Array)
worst_case_input = sorted(best_case_input)

# Measure Time for Best Case
start_time = time.time()
merge_sort(best_case_input)
best_case_time = time.time() - start_time

# Measure Time for Worst Case
start_time = time.time()
merge_sort(worst_case_input)
worst_case_time = time.time() - start_time

# Results
print(f"Best Case Time: {best_case_time:.6f} seconds")
print(f"Worst Case Time: {worst_case_time:.6f} seconds")

#---------------
def crossover(parent1, parent2):
    size = len(parent1)
    start, end = sorted(random.sample(range(size), 2))

    # Create offspring with None placeholders
    offspring = [None] * size

    # Copy segment from Parent 1
    offspring[start:end] = parent1[start:end]

    # Fill the remaining cities from Parent 2
    p2_index = end
    for i in range(size):
        if offspring[i] is None:
            while parent2[p2_index % size] in offspring:
                p2_index += 1
            offspring[i] = parent2[p2_index % size]

    return offspring

# Example Chromosomes (Routes)
parent1 = [0, 1, 2, 3, 4, 5, 6, 7]
parent2 = [4, 5, 6, 7, 0, 1, 2, 3]

# Perform Crossover
offspring = crossover(parent1, parent2)

# Results
print("Parent 1:  ", parent1)
print("Parent 2:  ", parent2)
print("Offspring: ", offspring)

"""Time Complexity: O(nlogn)
Space Complexity: O(n)
Complexity Analysis for Crossover
Time Complexity:
Filling offspring requires O(n) operations to check for duplicates and map values.
Total Time Complexity: O(n).
Space Complexity:
Space is required for the offspring and auxiliary structures.
Total Space Complexity: O(n).

Implement solution to 0/1 Knapsack using Dynamic Programming algorithm.
"""

def knapsack_top_down(weight, value, capacity, n):
    if n == 0 or capacity == 0:
        return 0

    if t[n][capacity] != -1:
        return t[n][capacity]

    if weight[n-1] <= capacity:
        t[n][capacity] = max(
            value[n-1] + knapsack_top_down(weight, value, capacity-weight[n-1], n-1),
            knapsack_top_down(weight, value, capacity, n-1))
        return t[n][capacity]
    else:
        t[n][capacity] = knapsack_top_down(weight, value, capacity, n-1)
        return t[n][capacity]

n = int(input("Enter the number of items: "))

profits = []
weights = []

print("Enter the profits and weights of the items:")
for i in range(n):
    p = int(input(f"Profit of item {i+1}: "))
    w = int(input(f"capacityeight of item {i+1}: "))
    profits.append(p)
    weights.append(w)

capacity = int(input("Enter the capacity of the knapsack: "))

t = [[-1 for _ in range(capacity + 1)] for _ in range(n + 1)]

max_profit = knapsack_top_down(weights, profits, capacity, n)
print(f"The maximum profit that can be obtained is: {max_profit}")

"""---

Implement N queens problem using Backtracking. Compare the time required for 4,5,6,7, and 8 Queens.
"""

import time

# Function to check if placing a queen is safe
def is_safe(board, row, col, n):
    # Check column
    for i in range(row):
        if board[i] == col:
            return False

    # Check upper left diagonal
    for i, j in zip(range(row, -1, -1), range(col, -1, -1)):
        if board[i] == j:
            return False

    # Check upper right diagonal
    for i, j in zip(range(row, -1, -1), range(col, n)):
        if i >= 0 and board[i] == j:
            return False

    return True

# Recursive backtracking function
def solve_n_queens(board, row, n, solutions):
    if row == n:  # All queens are placed
        solutions.append(board[:])
        return

    for col in range(n):  # Try placing queen in each column
        if is_safe(board, row, col, n):
            board[row] = col
            solve_n_queens(board, row + 1, n, solutions)
            board[row] = -1  # Backtrack

# Function to solve N-Queens and measure time
def n_queens(n):
    board = [-1] * n
    solutions = []
    start_time = time.time()
    solve_n_queens(board, 0, n, solutions)
    end_time = time.time()
    return len(solutions), end_time - start_time

# Test for N = 4 to 8 and compare time
for n in range(4, 9):
    num_solutions, elapsed_time = n_queens(n)
    print(f"N = {n}, Solutions = {num_solutions}, Time = {elapsed_time:.6f} seconds")

"""---

**5.Implement TSP using LCBB(Least Cost Branch & Bound)**
"""

import numpy as np
import heapq

# Node class for each state in the search tree
class Node:
    def __init__(self, level, path, reduced_matrix, cost, bound):
        self.level = level  # Number of cities visited so far
        self.path = path    # Current path taken (sequence of cities)
        self.reduced_matrix = reduced_matrix  # Current reduced cost matrix
        self.cost = cost    # Total cost incurred so far
        self.bound = bound  # Lower bound of the current path

    def __lt__(self, other):
        return self.bound < other.bound  # Priority based on bound (lower is better)

# Function to reduce the matrix and calculate the reduction cost
def reduce_matrix(matrix):
    reduced_matrix = matrix.copy()

    # Row reduction: reduce each row by its minimum element
    row_min = np.min(reduced_matrix, axis=1)
    row_min[row_min == np.inf] = 0  # Don't reduce rows of all inf
    reduced_matrix -= row_min[:, np.newaxis]  # Subtract row min from each element in row

    # Column reduction: reduce each column by its minimum element
    col_min = np.min(reduced_matrix, axis=0)
    col_min[col_min == np.inf] = 0  # Don't reduce cols of all inf
    reduced_matrix -= col_min  # Subtract col min from each element in col

    # Total reduction cost is the sum of row and column reductions
    reduction_cost = np.sum(row_min) + np.sum(col_min)
    return reduced_matrix, reduction_cost

# Calculate the bound (lower bound) for a node
def calculate_bound(matrix, current_cost):
    reduced_matrix, reduction_cost = reduce_matrix(matrix)
    return reduced_matrix, current_cost + reduction_cost

# Generate children of a node (new paths)
def generate_children(node, n, cost_matrix):
    children = []
    for i in range(n):
        if i not in node.path:  # Visit an unvisited city
            new_path = node.path + [i]
            new_matrix = node.reduced_matrix.copy()

            # Mark visited row (from) and column (to) as infinity
            new_matrix[node.path[-1], :] = np.inf
            new_matrix[:, i] = np.inf
            new_matrix[i][0] = np.inf  # Set return to start as infinity temporarily

            # Calculate new cost using the original cost matrix (not the reduced one)
            new_cost = node.cost + cost_matrix[node.path[-1], i]
            reduced_matrix, bound = calculate_bound(new_matrix, new_cost)
            children.append(Node(node.level + 1, new_path, reduced_matrix, new_cost, bound))

    return children

# Main TSP function using Branch and Bound
def tsp_branch_and_bound(cost_matrix):
    n = cost_matrix.shape[0]

    # Reduce initial matrix and calculate the starting bound
    reduced_matrix, initial_bound = reduce_matrix(cost_matrix)

    # Initialize the root node (starting at city 0)
    root = Node(0, [0], reduced_matrix, 0, initial_bound)
    pq = []
    heapq.heappush(pq, root)
    best_cost = np.inf
    best_path = None

    # Branch and Bound search
    while pq:
        node = heapq.heappop(pq)

        # If the current bound exceeds the best cost found so far, prune the node
        if node.bound >= best_cost:
            continue

        # If this node is a solution (all cities visited), check the total cost
        if node.level == n - 1:
            # Complete the tour by returning to the starting city (0)
            final_cost = node.cost + cost_matrix[node.path[-1], 0]
            if final_cost < best_cost:
                best_cost = final_cost
                best_path = node.path + [0]  # Add the return to the starting city
            continue

        # Generate child nodes (branching)
        children = generate_children(node, n, cost_matrix)
        for child in children:
            if child.bound < best_cost:  # Only explore promising children
                heapq.heappush(pq, child)

    return best_path, best_cost

# Example usage:
cost_matrix = np.array([[np.inf, 10, 15, 20],
                        [10, np.inf, 35, 25],
                        [15, 35, np.inf, 30],
                        [20, 25, 30, np.inf]])

solution, cost = tsp_branch_and_bound(cost_matrix)
print(f"Optimal tour: {solution} with cost {cost}")

"""**6. Implement TSP using Genetic Algorithm**

"""

import random
import math

# Define constants
INT_MAX = 2147483647
V = 5  # Number of cities in TSP
GENES = "ABCDE"  # Cities represented as letters
START = 0  # Starting city index
POP_SIZE = 10  # Population size for the algorithm

# Structure of an individual in the population
class individual:
    def __init__(self, gnome=None):
        self.gnome = gnome if gnome else self.create_gnome()  # Create a random gnome if not provided
        self.fitness = self.cal_fitness()  # Fitness value

    # Create a random gnome (tour)
    def create_gnome(self):
        gnome = "0"  # Start city is fixed at 0
        while len(gnome) < V:
            next_city = rand_num(1, V)  # Choose a random city between 1 and V-1
            if not repeat(gnome, chr(next_city + 48)):  # Avoid repeating cities
                gnome += chr(next_city + 48)
        gnome += gnome[0]  # Return to start city
        return gnome

    # Calculate fitness (distance of the tour)
    def cal_fitness(self):
        distance_matrix = [
            [0, 2, INT_MAX, 12, 5],
            [2, 0, 4, 8, INT_MAX],
            [INT_MAX, 4, 0, 3, 3],
            [12, 8, 3, 0, 10],
            [5, INT_MAX, 3, 10, 0]
        ]
        fitness = 0
        for i in range(len(self.gnome) - 1):
            from_city = ord(self.gnome[i]) - 48
            to_city = ord(self.gnome[i + 1]) - 48
            if distance_matrix[from_city][to_city] == INT_MAX:
                return INT_MAX  # Invalid path
            fitness += distance_matrix[from_city][to_city]
        return fitness

    def __lt__(self, other):
        return self.fitness < other.fitness

# Generate a random number within a range
def rand_num(start, end):
    return random.randint(start, end-1)

# Check if a city is repeated in the gnome
def repeat(gnome, ch):
    return ch in gnome

# Mutate a gene by swapping two cities
def mutatedGene(gnome):
    gnome = list(gnome)
    while True:
        r = rand_num(1, V)
        r1 = rand_num(1, V)
        if r1 != r:  # Ensure distinct swap positions
            gnome[r], gnome[r1] = gnome[r1], gnome[r]
            break
    return ''.join(gnome)

# Cooling function to reduce temperature
def cooldown(temp):
    return (90 * temp) / 100  # Reduce by 10%

# Function to run the TSP Genetic Algorithm
def TSPUtil():
    gen = 1  # Current generation
    gen_thres = 5  # Max number of generations
    temperature = 10000  # Initial temperature
    population = []

    # Initialize population with random gnomes
    for i in range(POP_SIZE):
        new_individual = individual()
        population.append(new_individual)

    print("\nInitial population: \nGNOME     FITNESS VALUE\n")
    for i in range(POP_SIZE):
        print(population[i].gnome, population[i].fitness)
    print()

    # Perform generations
    while temperature > 1000 and gen <= gen_thres:
        population.sort()  # Sort based on fitness
        print("\nCurrent temp: ", temperature)
        new_population = []

        # Create new generation by mutation
        for i in range(POP_SIZE):
            parent = population[i]
            while True:
                mutated_gnome = mutatedGene(parent.gnome)  # Mutate parent's gnome
                new_gnome = individual(mutated_gnome)  # Create new individual

                if new_gnome.fitness <= population[i].fitness:  # Accept if better
                    new_population.append(new_gnome)
                    break
                else:
                    # Accept with probability based on simulated annealing
                    prob = math.exp(-(new_gnome.fitness - population[i].fitness) / temperature)
                    if random.random() < prob:
                        new_population.append(new_gnome)
                        break

        temperature = cooldown(temperature)  # Reduce temperature
        population = new_population  # Update population with new generation
        print("Generation", gen)
        print("GNOME     FITNESS VALUE")
        for i in range(POP_SIZE):
            print(population[i].gnome, population[i].fitness)
        gen += 1

# Run the TSP algorithm
if __name__ == "__main__":
    TSPUtil()

import threading
import time
import random

class Philosopher(threading.Thread):
    def __init__(self, id, left_fork, right_fork):
        super().__init__()
        self.id = id
        self.left_fork = left_fork
        self.right_fork = right_fork

    def run(self):
        while True:
            self.think()
            self.eat()

    def think(self):
        print(f"Philosopher {self.id} is thinking.")
        time.sleep(random.uniform(1, 3))

    def eat(self):
        with self.left_fork:  # Pick up left fork
            with self.right_fork:  # Pick up right fork
                print(f"Philosopher {self.id} is eating.")
                time.sleep(random.uniform(1, 2))  # Simulate eating
                print(f"Philosopher {self.id} has finished eating.")

def dining_philosophers(n):
    forks = [threading.Semaphore(1) for _ in range(n)]  # One fork per philosopher
    philosophers = [
        Philosopher(i, forks[i], forks[(i + 1) % n])
        for i in range(n)
    ]

    # Start all philosopher threads
    for philosopher in philosophers:
        philosopher.start()

    # Allow philosophers to run indefinitely
    for philosopher in philosophers:
        philosopher.join()

if __name__ == "__main__":
    dining_philosophers(5)

"""Key Features in the Solution
Concurrency:

Each philosopher runs on a separate thread.
Access to shared forks is controlled using semaphores.
Fairness:

No philosopher holds onto a fork indefinitely.
Each philosopher gets a chance to eat as soon as forks are available.
Deadlock Prevention:

Fork acquisition is ordered, ensuring no circular wait condition (essential to prevent deadlock).
Starvation Prevention:

Philosophers use a fair locking mechanism (FIFO-based semaphore) to avoid any philosopher being perpetually denied resources.


### **Complexity Analysis of the Dining Philosopher Problem Solution**

---

### **Time Complexity**
1. **Acquiring Forks**:
   - Each philosopher must acquire two semaphores (one for each fork). Semaphore acquisition is an \(O(1)\) operation.
   - **Per philosopher**: \(O(1) + O(1) = O(1)\).

2. **Eating and Thinking**:
   - Simulated with a sleep function, whose duration depends on the system (not computational complexity).
   - This part is \(O(1)\) for computational purposes.

3. **Overall Time Complexity**:
   - For \(N\) philosophers, time complexity per cycle (think + eat): \(O(N)\) as each philosopher performs independent operations.
   - **Total Time Complexity**: \(O(N \cdot C)\), where \(C\) is the number of cycles (iterations).

---

### **Space Complexity**
1. **Forks**:
   - Each philosopher uses a semaphore for the fork, requiring \(O(N)\) memory for \(N\) philosophers.

2. **Threads**:
   - Each philosopher runs on a separate thread. Thread creation requires constant memory overhead.
   - **Space for threads**: \(O(N)\).

3. **Data Structures**:
   - No additional data structures are used apart from the semaphores and threads.

4. **Overall Space Complexity**:
   - **Space Complexity**: \(O(N)\).

# Implement multithreaded matrix multiplication. And compare the time taken with
# sequential matrix multiplication.
"""

import numpy as np
import time

def sequential_matrix_multiplication(A, B):
    rows_A, cols_A = A.shape
    rows_B, cols_B = B.shape
    result = np.zeros((rows_A, cols_B))

    for i in range(rows_A):
        for j in range(cols_B):
            result[i, j] = np.dot(A[i, :], B[:, j])
    return result

# Example usage
A = np.random.rand(500, 500)
B = np.random.rand(500, 500)

# Timing sequential multiplication
start_time = time.time()
sequential_result = sequential_matrix_multiplication(A, B)
sequential_time = time.time() - start_time
print(f"Sequential matrix multiplication time: {sequential_time} seconds")


import concurrent.futures

def worker(A, B, result, row, col):
    result[row, col] = np.dot(A[row, :], B[:, col])

def multithreaded_matrix_multiplication(A, B):
    rows_A, cols_A = A.shape
    rows_B, cols_B = B.shape
    result = np.zeros((rows_A, cols_B))

    with concurrent.futures.ThreadPoolExecutor() as executor:
        futures = []
        for i in range(rows_A):
            for j in range(cols_B):
                futures.append(executor.submit(worker, A, B, result, i, j))

        for future in concurrent.futures.as_completed(futures):
            future.result()

    return result

# Timing multithreaded multiplication
start_time = time.time()
multithreaded_result = multithreaded_matrix_multiplication(A, B)
multithreaded_time = time.time() - start_time
print(f"Multithreaded matrix multiplication time: {multithreaded_time} seconds")

"""Time Complexity:Ideally, the time complexity will be
ð‘‚
(
ð‘›
3
/
ð‘
)
O(n
3
 /p), where
ð‘
p is the number of threads.

Space Complexity:
Same as sequential:
ð‘‚
(
ð‘›
2
)
O(n
2
 ), since we still need space for the result matrix
ð¶
C

For implementing Travelling Salesman problem using LC branch and bound technique, design

the following 3 functions
i) Generate the start (first) node for the LCBB TSP
ii) Generate all the children of a given node
iii) To check whether a given node is a leaf node
"""

import numpy as np

class TSPNode:
    def __init__(self, path, cost, remaining_cities):
        self.path = path  # Path taken so far (a list of visited nodes)
        self.cost = cost  # Cost of the current path
        self.remaining_cities = remaining_cities  # Cities yet to be visited

def generate_start_node(n):
    # Start with an empty path, cost 0, and all cities remaining to visit (except the starting city)
    start_node = TSPNode(path=[0], cost=0, remaining_cities=set(range(1, n)))
    return start_node


def generate_children(node, dist_matrix):
    children = []
    for city in node.remaining_cities:
        # Create a new path and cost for the child node
        new_path = node.path + [city]
        new_cost = node.cost + dist_matrix[node.path[-1]][city]
        new_remaining_cities = node.remaining_cities - {city}

        # Create a new child node and add it to the list
        child_node = TSPNode(path=new_path, cost=new_cost, remaining_cities=new_remaining_cities)
        children.append(child_node)
    return children
def is_leaf_node(node):
    # A node is a leaf if there are no remaining cities to visit
    return len(node.remaining_cities) == 0
# Example distance matrix (distances between 4 cities)
dist_matrix = np.array([
    [0, 10, 15, 20],
    [10, 0, 35, 25],
    [15, 35, 0, 30],
    [20, 25, 30, 0]
])

# Generate the start node for a TSP problem with 4 cities
start_node = generate_start_node(4)

# Generate all children of the start node
children = generate_children(start_node, dist_matrix)

# Check if the start node is a leaf node (it should not be)
print(f"Is the start node a leaf? {is_leaf_node(start_node)}")

# Check if any of the children are leaf nodes
for child in children:
    print(f"Child path: {child.path}, Is it a leaf? {is_leaf_node(child)}")

"""For implementing solution to Travelling Salesman problem (TSP) using Genetic Algorithm,
design the following 2 functions.
i) Crossover of 2 chromosome representing solution of Traveling Salesperson
Problem (TSP)
ii) Selection of a solution from a population
"""

import numpy as np
import random

# Example distance matrix (for 5 cities)
dist_matrix = np.array([
    [0, 10, 15, 20, 25],
    [10, 0, 35, 25, 30],
    [15, 35, 0, 30, 15],
    [20, 25, 30, 0, 10],
    [25, 30, 15, 10, 0]
])

# Function to generate a random population of chromosomes (paths)
def generate_population(pop_size, num_cities):
    return [np.random.permutation(num_cities) for _ in range(pop_size)]

# Fitness function to evaluate the total path length
def fitness_function(chromosome, dist_matrix):
    cost = 0
    for i in range(len(chromosome) - 1):
        cost += dist_matrix[chromosome[i], chromosome[i + 1]]
    cost += dist_matrix[chromosome[-1], chromosome[0]]  # Add return to the start
    return cost

# Tournament Selection to choose a parent based on fitness
def tournament_selection(population, fitness, tournament_size=3):
    selected_indices = np.random.choice(len(population), tournament_size, replace=False)
    best_idx = selected_indices[0]

    for idx in selected_indices[1:]:
        if fitness[idx] < fitness[best_idx]:  # lower fitness value is better (shorter path)
            best_idx = idx

    return population[best_idx]

# Order 1 Crossover (OX1) to combine two parent chromosomes
def crossover(parent1, parent2):
    child1 = [-1] * len(parent1)
    child2 = [-1] * len(parent2)

    # Randomly select two points (start and end positions for crossover)
    start, end = sorted(np.random.choice(len(parent1), 2, replace=False))

    # Copy the segment from parent1 to child1 and from parent2 to child2
    child1[start:end+1] = parent1[start:end+1]
    child2[start:end+1] = parent2[start:end+1]

    # Fill in the remaining positions with genes from the other parent, preserving order
    def fill_remaining(child, parent, other_parent):
        pos = end + 1
        for i in range(len(parent)):
            if parent[i] not in child:
                # Find next available position in the child
                while child[pos % len(child)] != -1:
                    pos += 1
                child[pos % len(child)] = parent[i]
                pos += 1
        return child

    child1 = fill_remaining(child1, parent2, parent1)
    child2 = fill_remaining(child2, parent1, parent2)

    return child1, child2

# Mutation function (optional for this implementation)
def mutate(chromosome, mutation_rate=0.1):
    if random.random() < mutation_rate:
        i, j = random.sample(range(len(chromosome)), 2)
        chromosome[i], chromosome[j] = chromosome[j], chromosome[i]
    return chromosome

# Function to evolve the population over generations
def evolve_population(population, dist_matrix, tournament_size=3, mutation_rate=0.1):
    # Calculate fitness of the population
    fitness_values = [fitness_function(chrom, dist_matrix) for chrom in population]

    # Sort population by fitness (best solutions at the front)
    population = sorted(population, key=lambda x: fitness_function(x, dist_matrix))

    # Select parents and generate new population
    new_population = []
    while len(new_population) < len(population):
        parent1 = tournament_selection(population, fitness_values, tournament_size)
        parent2 = tournament_selection(population, fitness_values, tournament_size)

        # Perform crossover to create offspring
        child1, child2 = crossover(parent1, parent2)

        # Optional mutation
        child1 = mutate(child1, mutation_rate)
        child2 = mutate(child2, mutation_rate)

        # Add the offspring to the new population
        new_population.append(child1)
        new_population.append(child2)

    # Keep only the best individuals (elitism)
    new_population = sorted(new_population, key=lambda x: fitness_function(x, dist_matrix))[:len(population)]

    return new_population

# Main function to run the genetic algorithm for TSP
def genetic_algorithm_tsp(dist_matrix, population_size=10, num_generations=100, mutation_rate=0.1):
    num_cities = dist_matrix.shape[0]

    # Generate initial population
    population = generate_population(population_size, num_cities)

    # Evolve the population over generations
    for generation in range(num_generations):
        print(f"Generation {generation + 1}")

        # Evolve population
        population = evolve_population(population, dist_matrix, mutation_rate=mutation_rate)

        # Best solution in the current generation
        best_solution = population[0]
        best_fitness = fitness_function(best_solution, dist_matrix)

        print(f"Best solution in generation {generation + 1}: {best_solution}")
        print(f"Total cost: {best_fitness}")

    # Return the best solution after all generations
    best_solution = population[0]
    best_fitness = fitness_function(best_solution, dist_matrix)
    return best_solution, best_fitness

# Run the Genetic Algorithm on the TSP
best_solution, best_fitness = genetic_algorithm_tsp(dist_matrix)

print(f"\nBest solution: {best_solution}")
print(f"Total cost: {best_fitness}")

"""Distance Matrix: We use a 5x5 distance matrix to represent the distances between each pair of cities.
Population Initialization: A random population of possible solutions (chromosomes) is generated, where each solution is a permutation of city indices.
Fitness Function: The fitness function calculates the total cost of the path for a given chromosome. The lower the cost, the better the fitness.
Tournament Selection: Parents are selected from the population using tournament selection. A subset of the population is chosen at random, and the best individual (shortest path) is selected.
Crossover (Order 1 Crossover): Two parent chromosomes are crossed over to create two children. The crossover ensures that no city is repeated in the offspring.
Mutation: Mutation is applied to the offspring with a certain probability. In this case, two random cities are swapped in the chromosome.
Evolution: Over a number of generations, the population evolves by performing selection, crossover, and mutation. Elitism is used to preserve the best individuals.
Main Algorithm: The genetic algorithm runs for a specified number of generations and prints the best solution (shortest path) found.

Time Complexity:
Population Initialization:
ð‘‚
(
ð‘
)
O(N), where
ð‘
N is the number of cities (for generating random permutations).
Fitness Calculation:
ð‘‚
(
ð‘
)
O(N) per chromosome, as we compute the total path length.
Crossover:
ð‘‚
(
ð‘
)
O(N), as we copy segments of the parent chromosomes.
Selection:
ð‘‚
(
ð¾
)
O(K) for tournament selection, where
ð¾
K is the tournament size.
Mutation:
ð‘‚
(
1
)
O(1), as we swap two cities in the chromosome.
Overall Complexity: The complexity of one generation is
ð‘‚
(
ð‘
â‹…
ð‘ƒ
)
O(Nâ‹…P), where
ð‘ƒ
P is the population size, and
ð‘
N is the number of cities. If the number of generations is
ðº
G, the total complexity is
ð‘‚
(
ð‘
â‹…
ð‘ƒ
â‹…
ðº
)
O(Nâ‹…Pâ‹…G).
"""

